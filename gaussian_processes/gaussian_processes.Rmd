---
title: "Robust Gaussian Process Modeling"
author: "Michael Betancourt"
date: "September 2020"
bibliography: gaussian_processes.bib
csl: institute-of-mathematical-statistics.csl
link-citations: yes
linkcolor: blue
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    toc: TRUE
    toc_depth: 3
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA)
knitr::opts_knit$set(global.par = TRUE)
```


```{r, message=FALSE}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
parallel:::setDefaultClusterOptions(setup_strategy = "sequential")

util <- new.env()

par(family="CMU Serif", las=1, bty="l", cex.axis=1, cex.lab=1, cex.main=1,
    xaxs="i", yaxs="i", mar = c(5, 5, 3, 5))
```

Now we generate our data:

```{r gen-data}
n <- 30
x1 <- rnorm(n, mean = 0, sd = 5)
x2 <- x1 + rpois(n, lambda = 1)
x3 <- (x1-.7*x2)*(x1+.3*x2)
x4 <- sqrt(6*x3^2 + rexp(n, rate = 4))
y<- 3*x4^(.8) + rnorm(n, mean = 0, sd = .5)
data <- data.frame(x1, x2, x3, x4)
#head(data)
N = n
alpha_true <- 3
rho_true <- 5.5
```

and then pack everything together,

```{r}
simu_data <- list(alpha=alpha_true, rho=rho_true, N=N, x=data, D = 4)
```

In order to sample from the Gaussian process projected onto our covariate grid
we just need to construct the Gram matrix and then sample from the corresponding
multivariate normal random number generator.

```{r}
writeLines(readLines("stan_programs/simu.stan"))
```

```{r}
simu_fit <- stan(file='stan_programs/simu.stan', data=simu_data,
                 warmup=0, iter=4000, chains=1, seed=494838,
                 algorithm="Fixed_param", refresh=1000)
```

To visualize the sampled functions we'll use some visualization code in the
auxiliary `gp_utility.R` script.

```{r}
source('gp_utility.R', local=util)
```

```{r}
util$plot_gp_prior_quantiles(simu_fit, x1, "Marginal Quantiles")
```

```{r}
writeLines(readLines("stan_programs/simu_normal.stan"))
```

```{r}
sigma_true <- 2
simu_data$sigma <- sigma_true

set.seed(2595)
simu_fit <- stan(file='stan_programs/simu_normal.stan', data=simu_data,
                 warmup=0, iter=4000, chains=1, seed=494838,
                 algorithm="Fixed_param", refresh=40)
```

Now in addition the plotting some of the functions sampled from the Gaussian
process prior,

```{r}
source('gp_utility.R', local=util)
util$plot_gp_prior_realizations(simu_fit, x1, "Prior Realizations")
```

and the marginal quantile ribbons,

```{r}
util$plot_gp_prior_quantiles(simu_fit, x1, "Prior Marginal Quantiles")
```

we can also plot the marginal quantile ribbons of the prior predictive
distribution that convolves the Gaussian process prior with the observational
model.

```{r}
util$plot_gp_prior_pred_quantiles(simu_fit, x4, "Prior Predictive Marginal Quantiles")
```

Let's grab the first simulation for closer inspection.

```{r}
f <- extract(simu_fit)$f[1,]
y <- extract(simu_fit)$y[1,]

c_mid_teal="#487575"
for (i in 1:4){
  x = data[,i]
  print(y)
  print(x)
plot(x, f, type="l", lwd=2, xlab="x", ylab="y",
     xlim=c(-11, 11), ylim=c(-10, 10))
points(x, y, col="white", pch=16, cex=0.6)
points(x, y, col=c_mid_teal, pch=16, cex=0.4)
}
```

In practice we are often interested in the functional behavior beyond just the
observed covariate values.  To emulate this circumstance let's define a
measurement by the observations at just eleven evenly spaced points along
the covariate grid.

```{r}
observed_idx <- c( (0:10) + 2)
N_obs = length(observed_idx)
x_obs <- x[observed_idx]
y_obs <- y[observed_idx]
```

The entire grid, however, will be considered for predictions.

```{r}
N_predict <- N
x_predict <- data
y_predict <- y

plot(x, f, type="l", lwd=2, xlab="x", ylab="y",
     xlim=c(-11, 11), ylim=c(-10, 10))
#points(x_predict, y_predict, col="white", pch=16, cex=0.6)
#points(x_predict, y_predict, col=c_mid_teal, pch=16, cex=0.4)
points(x_obs, y_obs, col="white", pch=16, cex=1.2)
points(x_obs, y_obs, col="black", pch=16, cex=0.8)
```

In order to use this simulation in the next section we'll put everything in a
safe place.

```{r}
stan_rdump(c("N_obs", "x_obs", "y_obs",
             "N_predict", "x_predict", "y_predict",
             "observed_idx"), file="output/normal.data.R")

stan_rdump(c("f", "x"), file="output/gp.truth.R")
```

## Fitting A General Gaussian Process Posterior

Fitting the posterior of a Gaussian process model is also straightforward
using the Markov chain Monte Carlo capabilities of Stan.  Here we include all of
the function values of interest in the `parameters` block and then specify the
joint probability density function, including the contributions from the
projection of the Gaussian process prior model and the observational model at
the observed covariate values, in the `model` block.

Keep in mind that this model is using a fixed Gaussian process specified by the
exponentiated quadratic covariance function parameters passed in through the
`data` block.

```{r}
writeLines(readLines("stan_programs/fit_normal.stan"))
```

```{r}
truth <- read_rdump("output/gp.truth.R")

data <- read_rdump("output/normal.data.R")
data$alpha <- alpha_true
data$rho <- rho_true
data$sigma <- sigma_true
data$D <- 4
```

```{r, cache=TRUE, warning=FALSE, message=FALSE}
normal_fit <- stan(file='stan_programs/fit_normal.stan', data=data,
                   seed=599, refresh=10)
```

To check the fit let's load my `stan_utility.R` script for its diagnostic
functions.

```{r}
source('stan_utility.R', local=util)

writeLines(capture.output(util$check_n_eff(normal_fit))[1:5])
writeLines(capture.output(util$check_rhat(normal_fit))[1:5])
util$check_div(normal_fit)
util$check_treedepth(normal_fit)
util$check_energy(normal_fit)
```

Unfortunately the diagnostics clearly indicate that Stan is not able to
accurately quantify the posterior for the projected function values.  We may be
able to moderate the computational problems, however, by employing a
_non-centered parameterization_ of the function values.  A non-centered
parameterization takes advantage of the fact that
$$
\mathbf{f} \sim \text{multi-normal} ( \boldsymbol{\mu}, \boldsymbol{\Sigma})
$$
is equivalent to
$$
\begin{align*}
\tilde{\mathbf{f}} &\sim \text{multi-normal} (\mathbf{0}, \mathbf{I})
\\
\mathbf{f} &= \boldsymbol{\mu} + \mathbf{L} \cdot \tilde{\mathbf{f}}
\end{align*}
$$
where $\mathbf{L}$ is the Cholesky factor of the covariance matrix,
$$
\boldsymbol{\Sigma} = \mathbf{L} \mathbf{L}^{T}.
$$
Parameterizing the latent functions directly or through the non-centered
parameterization specifies the same probability distribution, but the latter
enjoys a nicer posterior geometry when the likelihood function is relatively
diffuse.

The non-centered parameterization, and the recovery of the centered function
values, is straightforward to implement in Stan.

```{r}
writeLines(readLines("stan_programs/fit_normal_ncp.stan"))
```

```{r, cache=TRUE}
normal_fit <- stan(file='stan_programs/fit_normal_ncp.stan', data=data,
                   seed=5838298, refresh=10)
```

Not only does this run significantly faster but also there are no indications of
inaccurate computation.

```{r}
util$check_all_diagnostics(normal_fit)
```

Confident in our computation we can analyze the posterior, first with a
spaghetti plot.  Note that the true function doesn't look out of place compared
to the included posterior realizations.

```{r}
util$plot_gp_post_realizations(normal_fit, data, truth,
                               "Posterior Realizations")
```

We can also consider a ribbon plot, although we have to be a bit more careful
with the interpretation.  Because we're looking only at marginal quantiles the
ribbons won't properly capture correlations between the function values and
consequently will appear narrow than they should be.  The fact that our ribbons
span only 10% to 90% quantiles also visually suppresses the tails a bit;
estimating more extreme quantiles accurately would require running longer Markov
chains.  In any case when judging the posterior fit through these ribbons we
have to take these considerations into account not expect that the true function
be exactly contained within the ribbons.

```{r}
util$plot_gp_post_quantiles(normal_fit, data, truth,
                            "Posterior Marginal Quantiles")
```

Finally we can consider a ribbon plot of the posterior predictive distribution.
Comparing these ribbons to the observed data in black implements a posterior
_retrodictive_ check while comparing the ribbons to the held out data in teal
implements a posterior _predictive_ check.

Because posterior predictive intervals have no coverage guarantees in general
for either retrodictions or predictions, we can't quantify either check by
counting how many observations are within the quantile ribbons.  Instead this
visualization provides a qualitative check for any systematic deviations that
might indicate limitations of our Gaussian process model.

```{r}
util$plot_gp_post_pred_quantiles(normal_fit, data, truth,
                                 "Posterior Predictive Marginal Quantiles")
```

The advantage of probabilistic programming is now that we know how to implement
a Gaussian process in one generative model it becomes straightforward to
implement in another.  Here let's consider using a Gaussian process to model the
log intensity of a Poisson observational model.


```{r}
writeLines(readLines("stan_programs/fit_poisson_ncp.stan"))
```

```{r}
data <- read_rdump("output/poisson.data.R")
data$alpha <- alpha_true
data$rho <- rho_true
```

```{r, cache=TRUE}
poisson_fit <- stan(file='stan_programs/fit_poisson_ncp.stan', data=data,
                    seed=5838298, refresh=10)
```

The fit looks good.

```{r}
util$check_all_diagnostics(poisson_fit)
```

We can visualize the posterior over functions, which is no longer a Gaussian
process, using spaghetti plots

```{r}
util$plot_gp_post_realizations(poisson_fit, data, truth,
                               "Posterior Realizations")
```

or ribbon plots.

```{r}
util$plot_gp_post_quantiles(poisson_fit, data, truth,
                            "Posterior Marginal Quantiles")
```

Visual posterior predictive and retrodictive checks proceed in exactly the same
was as before.

```{r}
util$plot_gp_post_pred_quantiles_disc(poisson_fit, data, truth,
                                      "Posterior Predictive Marginal Quantiles")
```

## Simulating From An Analytic Gaussian Process Posterior

When the observational model is specified by a family of normal probability
density functions we can also take advantage of the analytic Gaussian process
posterior covariance function and conditional linear algebra operations to
exactly sample function values conditioned on the observed data.

Instead of adding a nugget to the diagonal elements of the Gram matrix we add
the square of the measurement variability and then perform all of the necessary
linear algebra in the `gp_pred_rng` function.

```{r}
writeLines(readLines("stan_programs/fit_normal_anal.stan"))
```

Because we can sample the posterior function values exactly we no longer need
to run Markov chain Monte Carlo and can instead use the `Fixed_param` sampler.

```{r}
data <- read_rdump("output/normal.data.R")
data$alpha <- alpha_true
data$rho <- rho_true
data$sigma <- sigma_true

normal_fit_anal <- stan(file='stan_programs/fit_normal_anal.stan', data=data,
                        iter=4000, warmup=0, chains=1, seed=5838298,
                        algorithm="Fixed_param", refresh=1000)
```

This yields equivalent results,

```{r}
par(mfrow=c(1, 2))
util$plot_gp_post_quantiles(normal_fit, data, truth,
                            "MCMC Posterior Quantiles")
util$plot_gp_post_quantiles(normal_fit_anal, data, truth,
                            "MC Posterior Quantiles")
```

but at much lower computational cost.

# Inferring A Gaussian Process

In the previous section we inferred functional behavior assuming a fixed
Gaussian process, in particular a fixed Gaussian process specified by particular
exponentiated quadratic parameter values.  For most applications, however, we
won't have enough domain expertise to specify any single Gaussian process a
priori.  Instead we will need to _infer_ a Gaussian process within an
appropriate family.  More practically we will need to infer the covariance
function parameters that specify a unique Gaussian process, for example the
$\alpha$ and $\rho$ that specify an element of the exponentiated quadratic
family.  Here we will consider a normal observational model where we will also
have to infer the measurement variability $\sigma$.

Unfortunately inference within a family of Gaussian processes is frustrated by
degeneracies inherent to the flexibility that make Gaussian processes so useful
in practice.  In order to enable practical inference we need to consider
principled regularization that moderate these degeneracies without compromising
the utility of the resulting inferences.  Fortunately if we look carefully
enough we'll see that the structure of Gaussian processes naturally guides this
regularization.

In this section we'll see how these degeneracies manifest in the fragility of
point estimation before using full Bayesian inference to investigate the
nature of these pathologies.  Through that investigation we'll learn a little
bit about how Gaussian process inference interacts with the observed covariates
and then motivate prior modeling techniques that allow us to take full
advantage of exponentiated quadratic Gaussian processes in practice.

## Maximum Marginal Likelihood Estimation

Let's begin with point estimation where we endeavor to find and then use only a
single Gaussian process in our inferences.  In particular we'll use a _maximum
marginal likelihood estimator_ given by the exponentiated quadratic parameters
that optimize the likelihood of the observed data after the functional
behavior has been integrated out.

Note that maximum marginal likelihood estimators are subtly different from
maximum a posterior estimators that optimize the marginal posterior density
function of the exponentiated quadratic parameters.  The key difference arises
in the presence of constraints, such as the positivity of $\alpha$ and $\rho$.
While the maximum marginal likelihood, and hence maximum marginal likelihood
estimators, is invariant to unconstraining transformations the marginal
posterior density function, and hence maximum a posteriori estimators, is not.

We already implemented the marginal likelihood analytically in the
`fit_normal_anal.stan` program.  In order to fit the posterior covariance
function parameters all we have to do is move them from the `data` block to the
`parameters` block.

```{r}
writeLines(readLines("stan_programs/fit_covar1.stan"))
```

The maximum marginal likelihood estimator, or an approximation thereof, is then
given by running Stan's optimizer.

```{r}
gp_opt1 <- stan_model(file='stan_programs/fit_covar1.stan')
opt_fit <- optimizing(gp_opt1, data=data, seed=5838298, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

The small $\sigma$ suggests that the optimal Gaussian process supports functions
flexible enough to capture most of the variation in the observed data.  In order
to investigate exactly how flexible those functions are we can generate
posterior samples conditioned on the maximum marginal likelihood parameters.

```{r}
pred_data <- list(alpha=alpha, rho=rho, sigma=sigma,
                  N_obs=data$N_obs, x_obs=data$x_obs, y_obs=data$y_obs,
                  N_predict=data$N_predict, x_predict=data$x_predict)
pred_opt_fit <- stan(file='stan_programs/fit_normal_anal.stan', data=pred_data,
                     iter=1000, warmup=0, chains=1, seed=5838298, refresh=1000,
                     algorithm="Fixed_param")
```

Unfortunately the resulting functional posterior concentrates around functions
that strongly fluctuate away from the true functional behavior.

```{r}
par(mfrow=c(1, 1))

util$plot_gp_post_realizations(pred_opt_fit, data, truth,
                               "Posterior Realizations")
```

```{r}
util$plot_gp_post_quantiles(pred_opt_fit, data, truth,
                            "Posterior Marginal Quantiles")
```

Comparing the observed data in black to the posterior predictive distribution we
can immediately see that the posterior retrodictive, or in sample, performance
is quite good.  At the same time, however, the large deviations between the
unobserved data in teal and the posterior predictive distribution indicate
terrible posterior predictive, or out of sample, performance.  In other words
the strong fluctuations in the functional posterior are characteristic of
overfitting to the observed data.

```{r}
util$plot_gp_post_pred_quantiles(pred_opt_fit, data, truth,
                                 "Posterior Predictive Marginal Quantiles")
```

One thing that we always have to consider when using point estimates is their
stability.  Unstable point estimates suggests a degenerate marginal likelihood
function that is poorly quantified by _any_ single point.

Let's try the fit again, only this time with a different random number generator
seed that will select a different starting point for the parameters.

```{r}
opt_fit <- optimizing(gp_opt1, data=data, seed=2384853, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

Although the estimated value for $\alpha$ is similar, the values for $\rho$ and
$\sigma$ have almost switched.  The larger value of $\sigma$ suggests that the
functional posterior will more closely resemble the Gaussian process prior which
will concentrate on wiggly functions due to the small value of $\rho$.  To
confirm this hypothesis we sample from the functional posterior as before.

```{r}
pred_data <- list(alpha=alpha, rho=rho, sigma=sigma,
                  N_obs=data$N_obs, x_obs=data$x_obs, y_obs=data$y_obs,
                  N_predict=data$N_predict, x_predict=data$x_predict)
pred_opt_fit <- stan(file='stan_programs/fit_normal_anal.stan', data=pred_data,
                     iter=1000, warmup=0, chains=1, seed=5838298, refresh=1000,
                     algorithm="Fixed_param")
```

Indeed the typical functions fluctuate more quickly and don't seem to be
strongly influenced by the observed data.

```{r}
util$plot_gp_post_realizations(pred_opt_fit, data, truth,
                               "Posterior Realizations")
```

The weak influence of the observed data is more evident in the ribbon plot.
Because of the large $\sigma$ and small $\rho$ the posterior concentrates on
functions that are only locally influenced by the observed data, returning to
behavior more typical of the Gaussian process prior inbetween.

```{r}
util$plot_gp_post_quantiles(pred_opt_fit, data, truth,
                            "Posterior Marginal Quantiles")
```

```{r}
util$plot_gp_post_pred_quantiles(pred_opt_fit, data, truth,
                                 "Posterior Predictive Marginal Quantiles")
```

The strong sensitivity of the maximum marginal likelihood estimator to the
initial condition suggests that the marginal marginal likelihood function itself
may be degenerate.  While we might be able to investigate that degeneracy from
a theory perspective we won't be able to investigate it empirically without
moving beyond point estimation.  To investigate the entire breadth of the
maximum marginal likelihood function we'll need Markov chain Monte Carlo.

## Exploring the Marginal Likelihood Function

As I discuss in my
[degeneracy case study](https://betanalpha.github.io/assets/case_studies/identifiability.html#4_Diagnosing_Degeneracies),
Markov chain Monte Carlo, in particular the stubborn effectiveness of
Hamiltonian Monte Carlo, is a powerful tool for investigating the features of a
likelihood function even if we aren't necessarily interested in Bayesian
inference.  Let's see what we can learn about the marginal likelihood function
of our Gaussian process model and why maximum marginal likelihood was so
unstable.

### Uniform Prior Model

Consider the Stan program that we used above which, without an explicitly
specified prior model, presumes a prior model specified by improper uniform
prior density functions.

```{r}
writeLines(readLines("stan_programs/fit_covar1.stan"))
```

We now let Hamiltonian Monte Carlo take the program for a spin.

```{r, cache=TRUE, warning=FALSE, message=FALSE}
fit <- stan(file='stan_programs/fit_covar1.stan', data=data,
            seed=5838298, refresh=1000)
```

Innocent and naive in our hopes for the fit, let's take a look at the
diagnostics.

```{r}
util$check_all_diagnostics(fit)
```

Oooph, something _really_ unpleasant is going on.

Let's examine the posterior scatter plots, being careful to separate the
divergent and non-divergent transitions.

```{r}
c_dark_trans <- c("#8F272780")
c_dark_highlight_trans <- c("#7C000080")
c_green_trans <- c("#00FF0080")

partition <- util$partition_div(fit)
div_params <- partition[[1]]
nondiv_params <- partition[[2]]

par(mfrow=c(1, 3))

plot(log(nondiv_params$rho), log(nondiv_params$alpha),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(alpha)")
points(log(div_params$rho), log(div_params$alpha),
      col=c_green_trans, pch=16, cex=0.8)

plot(log(nondiv_params$rho), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(sigma)")
points(log(div_params$rho), log(div_params$sigma),
      col=c_green_trans, pch=16, cex=0.8)

plot(log(nondiv_params$alpha), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(alpha)", ylab="log(sigma)")
points(log(div_params$alpha), log(div_params$sigma),
      col=c_green_trans, pch=16, cex=0.8)
```

While the values of $\sigma$ look reasonable, the values of $\alpha$ are
unusually large and the values of $\rho$ are absolutely massive.  In fact
$$
\exp(700) \approx 10^{304}
$$
just about saturates the maximum value of the double precision floating point
numbers used by Stan.  In other words the Markov chains are concentrating around
infinity as much as the floating point numbers will allow them!

This behavior is typical of uniform density functions -- uniform is better
thought of as" "concentrating around infinity" rather that "flat" -- but the
fact that this behavior manifests in the posterior density function indicates
that the marginal likelihood function is itself flat, at least for large enough
values of $\rho$.

In order to understand why the marginal likelihood function is degenerate for
large values of $\rho$ let's take another look at the observed data.

```{r}
par(mfrow=c(1, 1))

plot(1, type="n", xlab="x", ylab="y", main="", xlim=c(-11, 11), ylim=c(-10, 10))
points(data$x_obs, data$y_obs, col="white", pch=16, cex=1.2)
points(data$x_obs, data$y_obs, col="black", pch=16, cex=0.8)
grid <- sapply(0:10, function(n) abline(v=2 * n - 10, col="gray80", lwd=2, lty=3))
```

Recall that because the exponentiated quadratic covariance function is
stationary the correlations between function values are determined entirely by
the distances between covariate values.  In particular we can learn about the
length scale $\rho$ only through the distances between the covariates in our
observed data.  Because our covariate grid spans only the range
$-10 \le x \le 10$ our observed data _cannot inform length scales above 20_!

In other words the degeneracy in the marginal likelihood function that we've
encountered is a fundamental limitation of our experimental design.  There is
simply no information about length scales larger than the maximum distance
between the observed covariates.

### Containment Prior Model {#sec:containment_prior}

To ensure a well-defined posterior density function we need a prior model that
can suppress infinity and compensate for the degeneracy of the marginal
likelihood function.  The modeling question is exactly how strongly our domain
expertise allows us to suppress infinity along each parameter direction.

One particularly tempting strategy is to design a prior model that suppresses
length scales larger than the maximum distance in the observed data, exactly
compensating for the inherent degeneracy of the marginal likelihood function.
The problem with this approach, however, is that in general there is no reason
why our domain expertise would be related to the observed covariate values.  In
particular there is no reason why length scales larger than the the maximum
distance that we happen to observe are inconsistent with our domain expertise.

A principled prior model can consider only _how the covariates are observed_
rather than any particular values.  For example consider a fixed covariate grid
that has been carefully designed to be sensitive to the length scales of
interest.  This deliberate experimental design implies than length scales larger
than the span of the covariate grid actually are inconsistent with the domain
expertise that motivated the design, in which case we can use that span to set
the scale of our prior model for $\rho$.

Without that relationship a prior model based on the span of the covariates will
overfit to the initial data and prematurely limit the scope of future
inferences.  Even though the span of the initially observed covariates does not
inform larger length scales future observations may very well introduce more
distant covariates that do.  Without any generative process relating the
observed covariates to the length scale we will have to focus on eliciting
domain expertise about $\rho$ directly.

Understanding the fundamental degeneracy of the exponentiated quadratic marginal
likelihood function tells us what information we need in our prior model to
ensure well-behaved inferences.  We cannot, however, always assume that the
needed information can be elicited from our domain expertise.  In particular
sometimes we just don't have enough information to inform a sufficiently
well-behaved posterior distribution.

Here let's assume that the covariate grid was deliberately designed based on
our knowledge of the functional length scales so that a prior that suppresses
length scales larger than 20 is compatible with our domain expertise.
Specifically let's take a half-normal probability density function with scale
$$
\rho
\sim
\text{half-normal} \! \left( 0, \frac{\Delta x}{3} \right)
=
\text{half-normal} \! \left( 0, \frac{20}{3} \right),
$$
which puts around 1% of the prior mass above $\rho = 20$.

While we're considering more careful prior models let's also add prior density
functions for $\alpha$ and $\sigma$ that contain them around zero with scales
that are consistent with our hypothetical domain expertise,
$$
\begin{align*}
\alpha &\sim \text{half-normal}(0, 2)
\\
\sigma &\sim \text{half-normal}(0, 1).
\end{align*}
$$

This more thoughtful prior model is straightforward to incorporate into our
previous Stan program.  Here we'll also include simulations of the posterior
functional behavior.

```{r}
writeLines(readLines("stan_programs/fit_covar2.stan"))
```

```{r, warning=FALSE, message=FALSE}
fit <- stan(file='stan_programs/fit_covar2.stan', data=data,
            seed=5838298, refresh=1000)
```

Unfortunately the diagnostics indicate divergent iterations and incomplete
exploration of the posterior density function.  Our better prior model resolved
one issue only to uncover another, which unfortunately is a fact of life in
many analyses.

```{r}
util$check_all_diagnostics(fit)
```

Let's investigate the problem with scatter plots between the unconstrained
parameters separated by divergent and non-divergent transitions.

```{r}
partition <- util$partition_div(fit)
div_params <- partition[[1]]
nondiv_params <- partition[[2]]

par(mfrow=c(1, 3))

plot(log(nondiv_params$rho), log(nondiv_params$alpha),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(alpha)")
points(log(div_params$rho), log(div_params$alpha),
      col=c_green_trans, pch=16, cex=0.8)

plot(log(nondiv_params$rho), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(sigma)")
points(log(div_params$rho), log(div_params$sigma),
      col=c_green_trans, pch=16, cex=0.8)

plot(log(nondiv_params$alpha), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(alpha)", ylab="log(sigma)")
points(log(div_params$alpha), log(div_params$sigma),
      col=c_green_trans, pch=16, cex=0.8)
```

While neither the $\rho$-$\alpha$ nor $\alpha$-$\sigma$ scatter plots look
ideal, the $\rho$-$\sigma$ scatter plot looks particularly pathological.  Once
$\log(\rho)$ drops below 0.7 or so the posterior samples suddenly change behavior
and disperse across the values of $\sigma$ consistent with the prior model.
Moreover the divergent transitions concentrate on the edge of that plateau.

```{r}
par(mfrow=c(1, 1))

plot(log(nondiv_params$rho), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(sigma)")
points(log(div_params$rho), log(div_params$sigma),
      col=c_green_trans, pch=16, cex=0.8)
abline(v=log(2), col="gray80", lwd=2, lty=3)
```

To better understand what's going on in that small length scale plateau let's
investigate some of the functional behavior in that region.

```{r}
util$plot_low_sigma_gp_post_realizations(fit, data, truth,
                                         "Posterior Realizations with sigma < 0.5")
```

The Gaussian processes in this plateau support functions that very closely,
if not exactly, interpolate through the observed data in which case the
measurement variability $\sigma$ is completely uninformed by the marginal
likelihood function.

One of the advantages of Bayesian inference is that the posterior distribution
aggregates both the more reasonable functional behavior above $\rho = 2$ with
the overfitting behavior below.

```{r}
util$plot_gp_post_realizations(fit, data, truth,
                               "Posterior Realizations")
```

In particular the better behaving configurations moderate the influence of
the overfitting model configurations, limiting the corruption of any well-posed
posterior expectation value.

```{r}
util$plot_gp_post_quantiles(fit, data, truth,
                            "Posterior Marginal Quantiles")
```

```{r}
util$plot_gp_post_pred_quantiles(fit, data, truth,
                                 "Posterior Predictive Marginal Quantiles")
```

Why does this overfitting behavior suddenly appear at $\log(\rho) \approx 0.7$,
or $\rho \approx 2$?  Let's look at our observed data again.

```{r}
plot(1, type="n", xlab="x", ylab="y", main="", xlim=c(-11, 11), ylim=c(-10, 10))
points(data$x_obs, data$y_obs, col="white", pch=16, cex=1.2)
points(data$x_obs, data$y_obs, col="black", pch=16, cex=0.8)
grid <- sapply(0:10, function(n) abline(v=2 * n - 10, col="gray80", lwd=2, lty=3))
```

The threshold $\rho = 2$ is exactly the _minimal_ distance between the observed
covariates.  Just as the data don't inform length scales larger than the
maximum covariate distance they also don't inform length scales _shorter_ than
the minimum covariate distance.  Gaussian processes with length scales shorter
than the minimal covariate distance support functions that are pinned near the
observed data but are poorly constrained elsewhere.

In other words we've encountered another limitation of the experimental design,
and all of the considerations of the previous section immediately apply.  To
prevent this degeneracy of the marginal likelihood function from propagating to
the posterior density function we need a prior model that suppresses Gaussian
processes with smaller length scales.  Whether or not we can do this in a
principled way depends on what domain expertise we can elicit.

### Informative Prior Model

Let's presume that the covariate grid was chosen deliberately and motivated by
domain expertise about both large _and_ small length scales.  In other words
we can reasonably build a prior model that suppresses both length scales above
20 and below 2.

While there are multiple families of prior density functions that suppress both
zero and infinity here we'll use an
[inverse gamma family](https://betanalpha.github.io/assets/case_studies/probability_densities.html#35_the_inverse_gamma_family).
The probability density functions in the inverse gamma family all feature
particularly strong suppression towards zero which is well-suited to moderating
the threshold behavior of the small length scale degeneracy.

We can select a particular element of the inverse gamma family by specifying
quantile constraints consistent with our domain expertise, for example
$$
\begin{align*}
0.01 = \mathbb{P}[0, l]
&= \int_{0}^{l} \mathrm{d}\rho \, \text{inv-gamma}(\rho \mid a, b)
\\
0.01 = \mathbb{P}[u, \infty] = 1 - \mathbb{P}[0, u]
&= 1 - \int_{0}^{u} \mathrm{d}\rho \, \text{inv-gamma}(\rho \mid a, b).
\end{align*}
$$
To solve for the parameters that identify the inverse gamma density function
with these tail quantiles we appeal to Stan's algebraic solver and a careful
initialization.

```{r}
writeLines(readLines("stan_programs/prior_tune.stan"))
```

```{r}
fit <- stan(file='stan_programs/prior_tune.stan', iter=1, warmup=0, chains=1,
            seed=5838298, algorithm="Fixed_param")
```

We don't have to be precious about the exact values so I'll round to the first
decimal place.

```{r}
c_dark <- c("#8F2727")
c_dark_highlight <- c("#7C0000")

lambda <- seq(0, 25, 0.001)
dens <- lapply(lambda, function(l) dgamma(1 / l, 4.6, rate=22.1) * (1 / l**2))
plot(lambda, dens, type="l", col=c_dark_highlight, lwd=2,
     xlab="lambda", ylab="Prior Density", yaxt='n')

lambda98 <- seq(2, 20, 0.001)
dens <- lapply(lambda98, function(l) dgamma(1 / l, 4.6, rate=22.1) * (1 / l**2))
lambda98 <- c(lambda98, 20, 2)
dens <- c(dens, 0, 0)

polygon(lambda98, dens, col=c_dark, border=NA)
```

Now let's update the prior contributions in our Stan program and see how we do.

```{r}
writeLines(readLines("stan_programs/fit_covar3.stan"))
```

```{r, warning=FALSE, message=FALSE}
fit <- stan(file='stan_programs/fit_covar3.stan', data=data,
            seed=5838298, refresh=1000)
```

Thankfully all of the diagnostic checks now pass without any indication of
fitting problems.

```{r}
util$check_all_diagnostics(fit)
```

By suppressing the degenerate plateaus at both small and large length scales
we're left with a reasonably well-behaved posterior density function that
doesn't frustrate the exploration of Hamiltonian Monte Carlo.

```{r}
partition <- util$partition_div(fit)
nondiv_params <- partition[[2]]

par(mfrow=c(1, 3))

plot(log(nondiv_params$rho), log(nondiv_params$alpha),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(alpha)")

plot(log(nondiv_params$rho), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(rho)", ylab="log(sigma)")

plot(log(nondiv_params$alpha), log(nondiv_params$sigma),
    col=c_dark_trans, pch=16, cex=0.8, xlab="log(alpha)", ylab="log(sigma)")
```

The posterior distribution for the Gaussian process and observational model
parameters also capture the true values from which the observed data were
simulated.

```{r}
params <- extract(fit)

c_light <- c("#DCBCBC")

par(mfrow=c(1, 3))

alpha_breaks=10 * (0:50) / 50 - 5
hist(params$alpha, main="", xlab="alpha", ylab="",
     col=c_dark, border=c_dark_highlight, yaxt='n')
abline(v=alpha_true, col=c_light, lty=1, lwd=3)

beta_breaks=10 * (0:50) / 50 - 5
hist(params$rho, main="", xlab="rho", ylab="",
     col=c_dark, border=c_dark_highlight, yaxt='n')
abline(v=rho_true, col=c_light, lty=1, lwd=3)

sigma_breaks=5 * (0:50) / 50
hist(params$sigma, main="", xlab="sigma", ylab="",
     col=c_dark, border=c_dark_highlight, yaxt='n')
abline(v=sigma_true, col=c_light, lty=1, lwd=3)
```

Similarly the functional posterior captures the true functional behavior.

```{r}
par(mfrow=c(1, 1))

util$plot_gp_post_realizations(fit, data, truth,
                               "Posterior Realizations")
```

```{r}
util$plot_gp_post_quantiles(fit, data, truth,
                            "Posterior Marginal Quantiles")
```

This all manifests in a posterior predictive data that covers both the observed
data _and_ the unobserved data exceptionally well.

```{r}
util$plot_gp_post_pred_quantiles(fit, data, truth,
                                 "Posterior Predictive Marginal Quantiles")
```

By heeding our computational diagnostics we were able to not only identify the
degeneracies in the marginal likelihood function but also confirm their origin
and work out an appropriate resolution with prior modeling.

## Back To The House That Karhunen–Loève Built

With a better understanding of the degeneracies inherent to inference over the
exponentiated quadratic family of Gaussian processes let's return to maximum
marginal likelihood and see if we can't do any better.

The plateaus in the marginal likelihood function at small and large length
scales help to explain why the optimization that we employed performed so
poorly.  Because the marginal likelihood function is constant in these regions
the gradients will vanish, and any gradient-based optimizer will either freeze
or continue on based on any inertia inherent to the algorithm.  Where the
optimizer encounters either plateau is highly sensitive to where the optimizer
is initialized, which manifests as the instability we saw above.

We can avoid these degenerate plateaus, however, by using our final prior
density function above as a penalty function in the optimization.  In Stan this
corresponds to using the same model block but switching the fitting algorithm to
optimization.  The only change we'll make to the Stan program is to remove the
predictions in the `generated quantities` for convenience.

```{r}
writeLines(readLines("stan_programs/fit_covar3_no_pred.stan"))
```

```{r}
gp_opt2<- stan_model(file='stan_programs/fit_covar3_no_pred.stan')
opt_fit <- optimizing(gp_opt2, data=data, seed=5838298, hessian=FALSE)


alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

While both $\alpha$ and $\rho$ are underestimated relative to the true values
of $3$ and $5.5$, respectively, $\sigma$ is quite close to the true value of
$2$.

More encouraging these estimates appear to be stable relative to random
initializations.

```{r}
opt_fit <- optimizing(gp_opt2, data=data, seed=95848338, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

```{r}
opt_fit <- optimizing(gp_opt2, data=data, seed=2485813, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

To see how well this Gaussian process captures the true functional behavior we
can generate posterior realizations as before.

```{r}
pred_data <- list(alpha=alpha, rho=rho, sigma=sigma,
                  N_obs=data$N_obs, x_obs=data$x_obs, y_obs=data$y_obs,
                  N_predict=data$N_predict, x_predict=data$x_predict)
pred_opt_fit <- stan(file='stan_programs/fit_normal_anal.stan', data=pred_data,
                     iter=1000, warmup=0, chains=1, seed=5838298, refresh=1000,
                     algorithm="Fixed_param")
```

Overall the functional posterior captures the true functional behavior
reasonably well.

```{r}
util$plot_gp_post_realizations(pred_opt_fit, data, truth,
                               "Posterior Realizations")
```

```{r}
util$plot_gp_post_quantiles(pred_opt_fit, data, truth,
                            "Posterior Marginal Quantiles")
```

That said we can see a mild pull away from the true functional behavior and
towards the observed data.

```{r}
util$plot_gp_post_pred_quantiles(pred_opt_fit, data, truth,
                                 "Posterior Predictive Marginal Quantiles")
```

While a strongly-informative penalty function does drastically improves maximum
marginal likelihood estimation, we have to be careful with the practicality of
this result.  Without having already quantified the entire posterior
distribution we never would have ben able to motivate the needed penalty
function in the first place.  Once we've quantified the posterior distribution
what additional utility does a point estimate offer?

## One Singular Fluctuation

Taking the derivation of principled regularization for granted we can still ask
what advantage, if any, Bayesian inference over the exponentiated quadratic
family has over properly-penalized point estimation.  Bayesian inference
inherently provides better uncertainty quantification the exponentiated
quadratic and observational model parameters than the maximum marginal
likelihood estimate, but what difference is there in the inference of the
_functional_ behavior?

Let's compare the two results side by side.

```{r}
par(mfrow=c(1, 2))

util$plot_gp_post_realizations(fit, data, truth,
                               "Posterior Realizations (Bayes)")

util$plot_gp_post_realizations(pred_opt_fit, data, truth,
                               "Posterior Realizations (MML)")
```

```{r}
util$plot_gp_post_quantiles(fit, data, truth,
                            "Posterior Marginal Quantiles (Bayes)")
                      
util$plot_gp_post_quantiles(pred_opt_fit, data, truth,
                            "Posterior Marginal Quantiles (MML)")
```

```{r}
util$plot_gp_post_pred_quantiles(fit, data, truth,
                                 "Posterior Predictive (Bayes)")
                           
util$plot_gp_post_pred_quantiles(pred_opt_fit, data, truth,
                                 "Posterior Predictive (MML)")
```

Although the differences are relatively small in this case the full Bayesian
inference captures more functional behavior that better contains the true
functional behavior, as well as providing more accurate predictions for both the
observed and unobserved data.  The superiority of averaging over the Gaussian
processes in the squared exponential instead of resorting to a single element is
no accident,  but rather a critical consequence of Gaussian processes
themselves.

To help understand why let's first consider the concept of the _support_ of a
probability distribution in a given parameterization of the ambient space.  The
support is defined as the smallest subset of the ambient space with probability
one.  Because conditions like "smallest" depend on the choice of
parameterization so too will the definition of support!  The support of a
probability distribution within a given parameterization also defines the range
of samples from that distribution.

For example consider a probability distribution specified by a normal
probability density function within a given parameterization of the real line.
The support in that parameterization is the entire real line, and a draw from
the corresponding normal random number generator can take values anywhere on the
real line.  If we reparameterize with an exponential transformation then the
distribution is specified by a log normal probability density function while the
support, and range of possible sample values, shrinks to the positive real line.

Similarly we can ask what the support of a Gaussian process might be.  By
definition the support has to be contained within the infinite-dimensional space
of all real-valued functions, but it's not clear how many functions the support
of any one Gaussian process actually encapsulates.  Remarkably the support of
any single Gaussian process is only an _infinitesimal_ slice of the space of all
functions.  Although that slice is still technically infinite-dimensional --
infinities are weird -- it encodes only a limited amount of possible functional
behaviors.

A related question is how much the support of different elements of the
exponentiated quadratic family overlap with each other.  The stronger the
overlap between Gaussian processes with similar marginal variation and length
scale parameters the less sensitive our inferences will be to the choice of any
single element, and the better point estimation will perform.  Unfortunately
the overlap is exactly zero, even for Gaussian processes with infinitesimally
close parameters.

In other words every exponentiated quadratic Gaussian process supports its own
distinct set of functional behaviors.  Consequently if we cannot infer the true
element exactly then any inference of the exponentiated quadratic family will
never be able to capture the true functional behavior.

Although this sounds dire the consequences are not _quite_ as dramatic in
practice.  The actual differences in functional behavior between different
exponentiated quadratic Gaussian processes with similar parameters is often
microscopic and unresolvable without nearly infinite amounts of data.  When we
average over the entire family of Gaussian processes, however, we aggregate
all of these microscopic differences into a significant increase in the
complexity of functional behavior captured in the model.

The utility of full Bayesian inference becomes particularly evident when the
regularization of the prior model isn't as strong as we might want it to be and
overfitting model configurations propagate to the posterior distribution like we
saw in [Section 3.2.2](#sec:containment_prior). While a point estimate might be
vulnerable to these overfitting model configurations any posterior expectation
will weight them against better behaved model configurations, moderating their
influence and yielding much more robust inferences.

# Advanced Topics

Although this case study is meant to serve as only a basic introduction to
Gaussian processes and their use in probabilistic modeling, I do want to touch
on a few advanced topics that one will eventually encounter when moving beyond
these basic applications.

## Additive Functional Models {#sec:additive}

One of the most common uses of Gaussian processes in probabilistic modeling is
not to model the entirety of a functional behavior but rather just the residual
behavior around simpler model.  Given the parametric function $f_{0}(x, \phi))$
we might model the functional behavior as
$$
f(x) = f_{0}(x, \phi) + f_{1}(x)
$$
where
$$
f_{1} \sim \text{GP}(0, k_{1}).
$$
If the residual behavior is particularly sophisticated we might add multiple
Gaussian processes,
$$
\begin{align*}
f(x) &= f_{0}(x, \phi) + \sum_{n} f_{n}(x)
\\
f_{n} &\sim \text{GP}(0, k_{n}),
\end{align*}
$$
where each component Gaussian process is responsible for modeling a particular
kind of functional behavior.  Because adding zero-mean Gaussian processes is
equivalent to adding a single zero-mean Gaussian process with an additive
covariance function,
$$
\begin{align*}
f_{n} &\sim \text{GP}(0, k_{n})
\\
\sum_{n} f_{n} &\sim \text{GP} \! \left( 0, \sum_{n} k_{n} \right)
\end{align*}
$$
these additive models are sometimes also referred to as additive covariance
function models.

For example consider modeling counts arising from some source whose intensity
is increasing with time, such as events in a growing population.  A simple model
might consider a Poisson observational model whose log intensity depends
linearly on time,
$$
pi(y; \alpha, \beta, t) = \text{Poisson}(y; \exp(\alpha + \beta \cdot t)).
$$
We can model a richer time dependence by adding a Gaussian process to the log
intensity,
$$
\begin{align*}
\log \lambda &= \alpha + \beta \cdot t + f(t)
\\
f &\sim \text{GP}(0, k).
\end{align*}
$$
If we want to model a more structured residual then we can add multiple
Gaussian processes,
$$
\begin{align*}
\log \lambda &= \alpha + \beta \cdot t + \sum_{n} f_{n}(t)
\\
f_{n} &\sim \text{GP}(0, k_{n}),
\end{align*}
$$
for example with each component Gaussian process responsible for modeling
residuals at different time scales.

The challenge with these models -- and really any additive model -- is ensuring
that the components do not support overlapping functional behavior.  If two
components can capture the same behavior then posterior inference will have to
explore both possibilities, inducing degenerate if not outright multimodal
posterior density functions.

In the Poisson model with
$$
\begin{align*}
\log \lambda &= \alpha + \beta \cdot t + f(t)
\\
f &\sim \text{GP}(0, k).
\end{align*}
$$
this requires that the Gaussian process specified by the covariance function
$k$ assigns little if any probability around functions that are approximately
linear in the span of the observed times.  When adding multiple Gaussian
processes together we also have to ensure that the functions supported by the
component Gaussian processes are sufficiently different.

If we don't know the covariance function for the residual Gaussian processes
exactly and instead want to infer them from within a family of Gaussian
processes then the only way that maintain the needed separation is through the
prior model over the family.  For example if we add multiple exponentiated
quadratic Gaussian processes together then we need to ensure that the prior
models for the marginal variation or length scale parameters have negligible
overlap.  In the Poisson time series example this might be achieved with the
prior model that concentrates each length scale parameter around a distinct
scale, such as
$$
\begin{align*}
\mathbb{P}[0.5 \, \mathrm{s} < \rho_{1} < 5 \, \mathrm{s}] &= 0.99
\\
\mathbb{P}[0.5 \, \mathrm{min} < \rho_{2} < 30 \, \mathrm{min} ] &= 0.99
\\
\mathbb{P}[0.5 \, \mathrm{hr} < \rho_{3} < \infty ] &= 0.99.
\end{align*}
$$
Ultimately Gaussian processes are like any other modeling technique -- they
perform best when used generatively rather than as a black box!

## High-Dimensional Covariate Spaces

Inferring functional behavior with Gaussian process priors can be considered a
form of _interpolation_.  The observational model constrains the functional
behavior _at_ the observed covariate values while the structure of the
covariance function constrains how that behavior propagates _between_ the
observed covariate values.

The less distance between the observed covariates the less work the Gaussian
process prior has to do and, generally, the more precise the inferred
interpolation will be.  When the observed covariates are far apart, however, the
interpolations become more uncertain unless the Gaussian process prior
compensates by concentrating on a smaller set of functional behaviors.

Unfortunately large distances are typical as the dimension of the covariate
space increases.  As I discuss in my
[probabilistic computation case study](https://betanalpha.github.io/assets/case_studies/probabilistic_computation.html#223_(d_rightarrow_infty)_is_the_loneliest_number)
even if we can maintain the distance along each dimension, the total distance
between any set of points in the covariate space will grow with the increasing
dimension.  Consequently for a fixed number of observed covariates values and a
fixed Gaussian process prior the interpolation uncertainty will also grow.

In order to maintain the same predictive power between the covariate values we
have to increase either the number of observed points or the domain expertise
encoded in the Gaussian process prior model, typically exponentially so in both
cases.  Because of the costly scaling of Gaussian process computations, we can
increase the number of covariate only so much in practice.  At the same time
we rarely have exponentially more domain expertise about the functional behavior
between the observed covariates just waiting to be exploited.

These challenges are not unique to Gaussian processes but are instead inherent
to any probabilistic interpolation method.  Interpolation of functional behavior
between observed points simply becomes a poorly-posed problem beyond a few
dimensions.  Ultimately interpolation methods like Gaussian processes are just
mechanisms to convert domain expertise into prior models for functional
behavior.  If we don't provide enough useful domain expertise then the resulting
prior models will be poorly informed no matter the details of that mechanism.
Unfortunately we rarely have much domain expertise about the behavior of
high-dimensional functions.


This has critical implications for _emulation_ methods that try to model the
behavior of expensive functions from a few costly evaluations.  In low
dimensions prior information about the functional behavior encoded in the
Gaussian process can lead to reasonably precise inferences both around and far
away from the evaluated points, but the model becomes less and less useful as
the dimension of the input space increases.  Probabilistic interpolation with
Gaussian processes might perform better than other interpolation methods against
this scaling, but ultimately there's only so much information that can be
exploited, and questions about the unknown function that can be answered.

## Approximating Gaussian Processes {#sec:approximations}

Because the practical implementation of Gaussian process is so expensive there
is a vast literature on Gaussian process _approximations_ that compromise some
of the structure of the covariance function in exchange for drastically reduced
cost.  The challenge with employing these approximations is ensuring that they
don't compromise so much of the structure that we loose the interpretability
that makes Gaussian process prior models so useful in modeling.

Principled approximations of Gaussian processes require a deeper understanding
of the theoretical foundations of Gaussian processes than what I have introduced
here.  Most of them use the theory to pivot from constructing the covariance
matrix of any finite dimensional projection to directly constructing the
corresponding _precision matrix_.  This dual approach not only can be cheaper
but also is better suited to sparse matrix approximations; the precision matrix
can often be well-approximated by a sparse matrix even when the covariance
matrix cannot.

The precision matrix can also be directly related to alternative representations
of Gaussian processes, such as _spectral densities_ and _stochastic differential
equations_.  Although these are more mathematically sophisticated
representations they can offer a wealth of understanding about the functional
behavior encoded within a Gaussian process, complementing the interpretability
of the covariance function representation.

For an example of the machinery required to project Matérn Gaussian processes
onto a set of finite basis functions, such as splines, see [@LindgrenEtAl:2011].

One helpful way of evaluating Gaussian process approximations is through
_backwards error analysis_.  Instead of trying to quantify the approximation
error directly we can sometimes identify the Gaussian process that is _exactly_
specified by an approximation and then compare that Gaussian process to the
desired Gaussian process that we are trying to approximate.  These comparisons
often provide more information about how much the structure of the Gaussian
process is compromised than many direct error quantifications.

# Conclusion

Gaussian processes provide a powerful yet principled technique for modeling
functional behavior, with applications spanning relatively simple regression
models to sophisticated generative models.  Incorporating entire families of
interpretable Gaussian processes offers even more modeling capabilities, but
only at the expense of degenerates once we condition on any finite set of
observations.  Fortunately principled prior modeling of these families can
suppress these pathologies, leaving us with rich models spanning a massive space
of well-behaved functional behaviors.

# Acknowledgements

I thank Dan Simpson, Aki Vehtari, Rob Trangucci, Maggie Lieu, Finn Lindgren,
and Jeremy Silver for helpful comments.

A very special thanks to everyone supporting me on Patreon: Aapeli Nevala,
abcnap, Abhinav Katoch, Abhishek Vasu, Adam Bartonicek, Al King, Alan O'Donnell,
Alex ANDORRA, Alexander Bartik, Alexander Noll, Allison M. Campbell,
Anders Valind, Andrea Serafino, Andrew Rouillard, Annie Chen, Anthony Wuersch,
Antoine Soubret, Arya, asif zubair, Austin Rochford, Austin Rochford,
Avraham Adler, Bao Tin Hoang, Ben Matthews, Ben Swallow, Benjamin Phillabaum,
Benoit Essiambre, Bo Schwartz Madsen, Brian Callander, Brian Hartley, Bryan Yu,
Cameron Smith, Canaan Breiss, Cat Shark, Chad Scherrer, Charles Naylor,
Chase Dwelle, Chibuikem Nwizu, Chris Zawora, Christopher Mehrvarzi,
Cole Monnahan, Colin Carroll, Colin McAuliffe, D, Damien Mannion, dan mackinlay,
Dan W Joyce, Daniel Hocking, Daniel Rowe, Darshan Pandit, David Burdelski,
David Humeau, David Pascall, Derek G Miller, Diogo Melo, Doug Rivers, Ed Berry,
Ed Cashin, Elizaveta Semenova, Erik Banek, Ero Carrera, Ethan Goan, Evan Russek,
Fabio Zottele, Federico Carrone, Felipe González, Fergus Chadwick, Filipe Dias,
Finn Lindgren, Florian Wellmann, Francesco Corona, Garrett Mooney, Gene K,
Geoff Rollins, George Ho, Georgia S, Granville Matheson, Guido Biele,
Guilherme Marthe, Hamed Bastan-Hagh, Hany Abdulsamad, Haonan Zhu, Ian ,
Ilaria Prosdocimi, Isaac S, J Michael Burgess, Jair Andrade Ortiz,
James McInerney, James Wade, JamesU, Janek Berger, Jeff Dotson, Jeff Helzner,
Jeffrey Arnold, Jeffrey Burnett, Jessica Graves, Joel Kronander, John Flournoy,
John Zito, Jonathan Sedar, Jonathan St-onge, Jonathon Vallejo, Jose Pereira,
Josh Weinstock, Joshua Mayer, Josué Mendoza, jsdenain , Justin Bois,
Juuso Parkkinen, Karim Naguib, Karim Osman, Kazuki Yoshida, Kejia Shi,
Kádár András, Lars Barquist, Leo Burgy, lizzie , Luiz Carvalho,
Lukas Neugebauer, Marc Dotson, Marc Trunjer Kusk Nielsen, Mark Donoghoe,
Markus P., Martin Modrák, Martin Rosecky, Matt Wescott, Matthew Kay,
Matthew Quick, Matthew T Stern, Maurits van der Meer, Maxim Kesin,
Michael Colaresi, Michael DeWitt, Michael Dillon, Michael Griffiths,
Michael Grosskopf, Michael Redman, Michael Tracy, Mikael Elhouar, Mike Lawrence,
Mikhail Popov, mikko heikkilä, MisterMentat , Márton Vaitkus, N Sanders,
Nathan Rutenbeck, Nerf-Shepherd , Nicholas Cowie, Nicholas Erskine,
Nicholas Knoblauch, Nicholas Ursa, Nick S, Nicolas Frisby, Noah Guzman, Okada,
Ole Rogeberg, Oliver Clark, Oliver Crook, Olivier Ma, Omri Har Shemesh,
Pablo León Villagrá, Patrick Boehnke, Pau Pereira Batlle, Paul Oreto,
Peter Smits, Pieter van den Berg , Pintaius Pedilici, Ravin Kumar, Reed Harder,
Riccardo Fusaroli, Richard Jiang, Richard Price, Robert Frost, Robert Goldman,
Robert kohn, Robin Taylor, Roofwalker, Scott Block, Sean Talts, Seth Axen,
Shira, sid phani, Simon Dirmeier, Simon Duane, Simon Lilburn, Srivatsa Srinath,
Stephanie Fitzgerald, Stephen Lienhard, Stephen Oates, Steve Bertolani, Stijn,
Stone Chen, Sue Marquez, Sus, Susan Holmes, Suyog Chandramouli, Sören Berg,
Teddy Groves, Teresa Ortiz, Thomas Littrell, Thomas Siegert, Thomas Vladeck,
Tiago Cabaço, Tim Howes, Tim Radtke, Tom McEwen, Tommy Naugle, Trey Spiller,
Tuan Nguyen Anh, Tyrel Stokes, Vanda Inacio de Carvalho, Vince, Virginia Webber,
vittorio trotta, Will Dearden, Will Farr, William Grosso, yolhaj , yureq , Z,
and ZAKH.

# References {-}

<div id="refs"></div>

# License {-}

A repository containing the material used in this case study is available on
[GitHub](https://github.com/betanalpha/knitr_case_studies/tree/master/gaussian_processes).

The code in this case study is copyrighted by Michael Betancourt and licensed
under the new BSD (3-clause) license:

https://opensource.org/licenses/BSD-3-Clause

The text and figures in this case study are copyrighted by Michael Betancourt
and licensed under the CC BY-NC 4.0 license:

https://creativecommons.org/licenses/by-nc/4.0/

# Original Computing Environment {-}

```{r}
writeLines(readLines(file.path(Sys.getenv("HOME"), ".R/Makevars")))
```

```{r}
sessionInfo()
```
